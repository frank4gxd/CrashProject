---
title: "milestone4"
author: "Frank Guo"
date: "2024-05-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
```
# Milestone 4 - Crashes Analysis
### By Xiaodong Guo

## 1. Goal.   

Our project has a significant objective- to construct models identifying the pivotal factors contributing to severe crashes. This crucial task is based on the data provided by the New Zealand Transport Agency(NZTA).    


## 2. Data Source.   

The original data, meticulously collected, originated from the Waka Kotahi NZ Transport Agency's open data portal(the tutor provided the link in the assignment piece). We specifically downloaded the dataset named  “Crash Analysis System (CAS) data” from the “Crash” catalogue, which encompasses all traffic crashes reported to us by the NZ Police. The data format is a “CVS” file. It was created on 3/25/2020 and last updated on 3/14/2024.    

The data includes crash datas from 2000 to 2023.    


## 3. Data Processing.   

#### Load Data.   


We load the data from csv flie. 
The dataset we got have 72 columns,and 821744 rows.
All the descriptions of attributes will be listed in  ***Appendix 4***.

```{r echo=FALSE,cache=TRUE}
library(tidyverse)
set.seed(230)
data <- read.csv("../data/Crash_Analysis_System_(CAS)_data.csv", header = TRUE, sep = ",")
dim(data)
```

#### 1.drop columns not related to our object.    


***We*** select following columns by common sense of mine.There are also have some description columns, that is long string values to desript an event or street name. That looks no sense,should drop them too. 

***Like*** "crashLocation1","crashLocation2", the location of crash is too detailed, we will keep region instead.    

***Also***, the column like "minorInjuryCount","seriousInjuryCount","fatalCount", they are highly related to define if the crash is severe. It is not superised that you will get more than 99% accuracy in prediction if these features are included. We will remove these columns too.    

***Compared*** crashYear with crashFinancialYear, the crashFinancialYear is more related to the domain business. So the crashFinalcialYear is kept.


```{r echo=TRUE,cache=TRUE,warning=FALSE}
columns_to_drop <- c("X","Y","OBJECTID","areaUnitID","crashDirectionDescription","","crashDistance",
                     "tlaId","tlaName","debris","meshblockId","northing","easting","crashLocation1",
                     "crashLocation2","directionRoleDescription","crashSHDescription","otherObject",
                     "phoneBoxEtc","minorInjuryCount","seriousInjuryCount","fatalCount","crashYear",
                     "objectThrownOrDropped")

data <- select(data, -one_of(columns_to_drop))
```


#### 2. dropping columns that all values are almost Null(more then 99% of the data is null).    


Columns like these are too sparse. The column names are crashRoadSideRoad" and "intersection".    

```{r echo=TRUE,cache=TRUE,warning=FALSE}
na_percentage <- colMeans(is.na(data))
columns_with_high_na <- names(na_percentage[na_percentage > 0.99])
print(columns_with_high_na)
data <- data %>% select(-columns_with_high_na)
```

#### 3. Define the target lable.    


Define crashSeverity == "Fatal Crash" and crashSeverity == "Serious Crash" as severe crashes given numeric value 1,Define crashSeverity == "Minor Crash" | crashSeverity == "Non-Injury Crash" as not severe crashes given numeric value 0.     

The ***"crashSeverity"*** Label will be the target label.     


```{r echo=TRUE,cache=TRUE,warning=FALSE}
table(data$crashSeverity)
```

After mapping:    



```{r echo=TRUE,cache=TRUE,warning=FALSE}
data <- data %>%
  mutate(crashSeverity = ifelse(crashSeverity == "Fatal Crash" | crashSeverity == "Serious Crash", 1,
                      ifelse(crashSeverity == "Minor Crash" | crashSeverity == "Non-Injury Crash", 0,
                             2))) %>%  filter(crashSeverity != 2)
table(data$crashSeverity)
```
     
     
***There*** are 767290 regular crashes and  54454 severe crashes.  Our target is to find the cause of severe crashes,but the severe observations' size is really small compares to the regular crashes. So we can't  use the whole dataset directly, it will overfit the majority(regular crashes) and underfit the minority(severe crashes), cause bias to majority,loss the importance information for our target(server crashes) inference.    


```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=4, fig.height=2}
library(ggplot2)

ggplot(data, aes(x = crashSeverity)) +
 geom_bar(fill = "blue") +
labs(title = "Histogram of crashes", x = "crashSeverity", y = "Frequency")

```
     

#### 4.Dealing with other predictors.   


***1. Attributes "weatherA" and "weatherB"***.   

They are String values could be treated as factors,not too many factors in each attribute, and the combinations also not too many factors but are more sensitive to understand the whole weather situation. In my opinion, these two could be combined as one attribute "weather",much easier to display and dealing with it later.    

Also For these two attributes, the Na value or String "None" are replaced as "Others" condition.What we got is like the output.    


```{r echo=TRUE,cache=TRUE,warning=FALSE}

data$weatherA <- ifelse(data$weatherA %in% c("None", "Null"), "Others", data$weatherA)
data$weatherB<- ifelse(data$weatherB %in% c("None", "Null"), "", data$weatherB)

data <- data %>% unite(weatherA,weatherB,col=weather,sep=" ")
table(data$weather)

```


***2. Deal with "region" and "holiday"***.   


***Replace*** the "Null","None" value in region with "Others".    

List all columns have the value "". That is holiday and regions.Replace the "" in other character attributes with "Others". The result will be used in Data Analysis block, so omitting the talbe() here.


```{r echo=TRUE,cache=TRUE,warning=FALSE}

empty_columns <- colnames(data)[apply(data == "", 2, any)]

# Print the empty columns
print(empty_columns)

data[data == ""] <- "Others"
```

     
     
***3. All the attributes with Na value***.    


***List*** all the other attributes with Na value,then check these attributs.

***According*** the descriptions of these attributes, we can use 0 to fill na value.Using 2 examples to explaint why 0 be used:    


***For***  "advisorySpeed" or "temporarySpeedLimit" attribute, the value is mean special speed limitation applied or advised in the road which is involed in the crash. use 0 here means no special speed limit applied(according the rode code, that is open road.follows open road speed limit).    


***For*** other attributes in the list, the value indicates the number of items involved in the crash. the Na value means no item(named by attribute name) is involved,that equals to 0.    


```{r echo=TRUE,cache=TRUE,warning=FALSE}
na_columns <- sapply(data, function(x) any(is.na(x)))
columns_with_na <- names(data)[na_columns]
print(columns_with_na)
data <- data %>%
  mutate_at(vars(one_of(columns_with_na)), ~replace_na(., 0))
```

Finally, all the Na or missing data is imputated and remedied.


### exploint data:     

      
1. at the financial year vs crashes, it is fluctuated, indicated some relations with the year,confirmed by Chi-test. By intuition and guess, maybe budget for traffic bureau matters. 

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
#data[] <- lapply(data, function(x) as.factor(x))
data$crashSeverity <- as.factor(data$crashSeverity)

group_by_fyear <- data %>% group_by(crashFinancialYear,crashSeverity) %>% summarise(count=n(),.groups="keep")

ggplot(group_by_fyear, aes(x = crashFinancialYear, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by financial year", x = "financial year", y = "count")+
  theme(axis.text.x = element_text(angle = 45, hjust =1))

chisq_test <- chisq.test(table(data$crashFinancialYear, data$crashSeverity))
print(chisq_test)
 # facet_wrap(~crashYear,nrow=3)
```


2. ***Comparing*** the crashes by region, Auckland region looks obviously high than others, considering of population density, it looks reasonable.While the ratio of severe crashes looks low. While regions like Gisbone,northland, southland, hawkesbay and westcoast looks has much hight ratio of severe crashes.    


```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
group_by_region <- data %>% group_by(region,crashSeverity) %>% summarise(count=n(),.groups="keep")

  ggplot(group_by_region, aes(x = region, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by region", x = "region", y = "count") +
  theme(axis.text.x = element_text(angle = 45, hjust =1))
```
     
     
3. ***Most*** of crashes happen in fine weather,and also, most of predictor attributes show strong skew. If we skip the fine weather stuation. It is clear that the light rain weather looks notable too. 

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
group_by_weather <- data %>% group_by(weather,crashSeverity) %>% summarise(count=n(),.groups="keep") 

ggplot(group_by_weather, aes(x = weather, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Weather Conditions", x = "Weather Type", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
      
      
4. ***Crashes*** happened all the light situation,the number of severe crashes looks almost the same in sunny ,dark or overcast.While in dark or twilight,the severe crashes ratio looks higher.     

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
group_by_light <- data %>% group_by(light,crashSeverity) %>% summarise(count=n(),.groups="keep") 
ggplot(group_by_light, aes(x = light, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "light Conditions", x = "light Type", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
  #facet_wrap(~region,nrow=6)
chisq_test <- chisq.test(table(data$light, data$crashSeverity))
print(chisq_test)
```
    
    
5. ***From*** the plot, we can tell that more bicycles involed in craches, more likly the crash to be a severe crash.Also, I skiped the bicycle = 1 and 0 because of the high proportion of the data at those values.

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=4, fig.height=3}

group_by_bicycle <- data %>% group_by(bicycle,crashSeverity) %>%
            summarise(count=n(), .groups = "keep")

ggplot(group_by_bicycle, aes(x = bicycle, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "bicycle ", x = "bicycle ", y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

6. ***As*** same as bicycles, from the Scatter plot, more pedestrians involved in the crash, the crash is more likely to be a several crash.I also skip the pedenstrain<2 in the plot.

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}

ggplot(data = data %>% filter(pedestrian>1), aes(x = pedestrian, y = crashSeverity)) +
  geom_jitter() +
  labs(title = "Scatter Plot of pedestrian vs. crashSeverity",
       x = "pedestrian",
       y = "crashSeverity")

#rm(data)
# group_by_pedestrian <- data %>% filter(pedestrian>0) %>% group_by(pedestrian,crashSeverity) %>% 
#             summarise(count=n(), .groups = "keep")
# 
# 
# # ggplot(group_by_pedestrian, aes(x = pedestrian, y = count, fill = crashSeverity)) +
# #   geom_bar(stat = "identity", position = "dodge") +
# #   labs(title = "pedestrian ", x = "pedestrian ", y = "Frequency") +
# #   theme_minimal() +
# #   theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
# # 

```
    
    
7. ***From*** the bar plot, we can see that the carStationWagon type of car is the most related car type in crashes. By chi-test, it shows strong relations with crashes.

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
# Assuming df is your data frame and attr1, attr2, attr3 are the attributes
carStationWagon <- sum(data$carStationWagon, na.rm = TRUE)
otherVehicleType <- sum(data$otherVehicleType, na.rm = TRUE)
suv <- sum(data$suv, na.rm = TRUE)
truck <- sum(data$truck, na.rm = TRUE)
vanOrUtility <- sum(data$vanOrUtility, na.rm = TRUE)


# Create a new data frame
df_sum <- data.frame(Attribute = c("carStationWagon", "otherVehicleType", "suv","truck","vanOrUtility"),
                     Sum = c(carStationWagon, otherVehicleType, suv,truck,vanOrUtility))

# Load the ggplot2 package
library(ggplot2)

# Create a bar plot
ggplot(df_sum, aes(x = Attribute, y = Sum)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  labs(x = "type of vehicle", y = "count involved in crashes", title = "Comparison of vehicle")

# Chi-square test 
chisq_test <- chisq.test(table(data$carStationWagon, data$crashSeverity))
print(chisq_test)

```


## 5. Analytical Plan.   

### Sampling Strategy.    


***Because*** of the imbalance of the dataset, I decisied to equally sample from severe crashes and regular crashes,that is ***20000*** observations from each type of crashes.
***We*** will use 80% of the sample data as training data, and 20% of the sample data as test data. To fit the model, we will factorise the charactor attributes,and numberic them.    


```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=5, fig.height=3}
#sample data

## balance sample
# Split data by class

class_data <- split(data, data$crashSeverity)

# Determine desired sample size (e.g., proportionally to the original class distribution)
desired_sample_size <- 20000  # Adjust as needed

# Sample each class
sampled_data <- lapply(class_data, function(class_subset) {
  # Determine the sample size for this class
  class_size <- nrow(class_subset)
  class_sample_size <- min(class_size, desired_sample_size)

  # Sample observations from this class
  sampled_indices <- sample(1:class_size, size = class_sample_size, replace = FALSE)

  # Return the sampled subset
  return(class_subset[sampled_indices, ])
})

# Combine sampled subsets
balanced_data <- do.call(rbind, sampled_data)
##

# try sample and smote, see what is the result
# set.seed(230)  # for reproducibility
# train_index <- sample(seq_len(nrow(data)), size = 0.7 * nrow(data))
# train_data <- data[train_index, ]
# test_data <- data[-train_index, ]
# 
# for (col in names(train_data)) {
#   if (is.character(train_data[[col]])) {
#     train_data[[col]] <- as.numeric(factor(train_data[[col]]))
#   }
# }
# 
# for (col in names(test_data)) {
#   if (is.character(test_data[[col]])) {
#     test_data[[col]] <- as.numeric(factor(test_data[[col]]))
#   }
# }
# 
# library(smotefamily)
# 
# # Separate features (X) and target variable (y) for training data
# X_train <- train_data[, !(names(train_data) == "crashSeverity")]  # Features (all columns except the target)
# y_train <- train_data$crashSeverity           # Target variable
# 
# # Separate features (X) and target variable (y) for testing data
# X_test <- test_data[, !(names(test_data) == "crashSeverity")]    # Features (all columns except the target)
# y_test <- test_data$crashSeverity              # Target variable
# 
# table(train_data$crashSeverity) 
# # Apply SMOTE for oversampling
# oversampled_data <- SMOTE(X = X_train, target = y_train)
# 
# # Convert oversampled data to data frame
# oversampled_data_df <- as.data.frame(oversampled_data$data)
# 
# # Combine oversampled data with original data
# combined_data <- cbind(oversampled_data_df, target = oversampled_data$crashSeverity)
# table(combined_data$crashSeverity)

```



```{r echo=FALSE,cache=TRUE,warning=FALSE}
#prepare training and test data

training_idx <- sample(nrow(balanced_data), nrow(balanced_data)*0.8)
test_idx <-(1:nrow(balanced_data))[-training_idx] 

training_data <- balanced_data[training_idx,]
test_data <- balanced_data[test_idx,]


for (col in names(training_data)) {
  if (is.character(training_data[[col]])) {
    training_data[[col]] <- as.numeric(factor(training_data[[col]]))
  }
}

for (col in names(test_data)) {
  if (is.character(test_data[[col]])) {
    test_data[[col]] <- as.numeric(factor(test_data[[col]]))
  }
}

#summary(training_data)
```



#### explore the sample data.   

***I*** use bicycle,carStationWagon,speedLimit,roadLane as samples to explore the distribution of the predictors. As we can see the most of same bicycle attribute value , having different target lable. It makes the inference be different, and indicates low accuracy of prediction. While carStationWagon shows some good trend to distinct the crashes compared to bicycle. 



```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=4, fig.height=3}
library(tidyverse)

group_by_bicycle <- balanced_data %>% group_by(bicycle,crashSeverity) %>% summarise(count=n(),.groups="keep")

ggplot(group_by_bicycle, aes(x = bicycle, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by bicycle", x = "bicycle", y = "count")+
  theme(axis.text.x = element_text(angle = 45, hjust =1))

group_by_carStationWagon <- balanced_data %>% group_by(carStationWagon,crashSeverity) %>% summarise(count=n(),.groups="keep")

ggplot(group_by_carStationWagon, aes(x =carStationWagon, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by carStationWagon ", x = "carStationWagon", y = "count")+
  theme(axis.text.x = element_text(angle = 45, hjust =1))


```

```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=4, fig.height=3}

group_by_speedLimit <- balanced_data %>% group_by(speedLimit,crashSeverity) %>% summarise(count=n(),.groups="keep")

ggplot(group_by_speedLimit, aes(x = speedLimit, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by speedLimit", x = "speedLimit", y = "count")+
  theme(axis.text.x = element_text(angle = 45, hjust =1))

group_by_roadLane <- balanced_data %>% group_by(roadLane,crashSeverity) %>% summarise(count=n(),.groups="keep")

ggplot(group_by_roadLane, aes(x =roadLane, y = count, fill = crashSeverity)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Comparison of crashes by roadLane ", x = "roadLane", y = "count")+
  theme(axis.text.x = element_text(angle = 45, hjust =1))


```

### Fitting Strategy         


  I fit the data using logistic regression, random forest and xgboost, to compare the performance using F1 score , accuracy ,TPR. Because we more focus on the factors of severe crashes,I applied 2 times heavier penality for those are severe crashes,but misclassified to archive higher TRP. 
  List the 15s important factors from the top as result for every fit.


#### 1. Fit logistic regression model.    


class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize severe crashes class more heavily.    


lr_model <- glm(crashSeverity ~ ., data = training_data, weights=class_weights, family = "binomial").    


Output is the confusion matrix, f1 score, accuracy, and write the importance list to importance_glm.csv.It is very similiar with the list by P values:     



```{r echo=FALSE,cache=TRUE,warning=FALSE}
# library(caret)
# 
# 
# test_Y <- test_data$crashSeverity
# 
# #test_X <- test_data %>% select(-crashSeverity)
# 
# test_lr <- test_data
# #class_weights <- ifelse(training_data$crashSeverity == 1, 3, 1)  # Penalize severe crashes class more heavily
# 
# lr_model <- glm(crashSeverity ~ ., data = training_data, family = "binomial")
# 
# print(lr_model)
# predictions <- predict(lr_model, newdata = test_lr, type = "response")
# binary_predictions <- ifelse(predictions >= 0.5, 1, 0)
# accuracy <- mean(binary_predictions == test_Y)
# 
# knitr::kable(table(binary_predictions,test_Y))
# 
# print("Accuracy is:")
# print(accuracy)
# 
# # Extract coefficients
# coefficients <- coef(lr_model)
# 
# # Calculate absolute values of coefficients
# abs_coefficients <- abs(coefficients)
# 
# # Create a dataframe to store coefficients and their absolute values
# coef_df <- data.frame(predictor = names(coefficients), coefficient = coefficients, abs_coefficient = abs_coefficients)
# 
# # Sort coefficients based on absolute values
# sorted_coef_df <- coef_df[order(abs_coefficients, decreasing = TRUE), ]
# 
# # Print sorted coefficients
# knitr::kable(head(sorted_coef_df,15))
# 
# 
# importance <- varImp(lr_model, scale = FALSE)
# print(importance)
# 


```



```{r echo=FALSE,cache=TRUE,warning=FALSE}
library(caret)


test_Y <- test_data$crashSeverity

#test_X <- test_data %>% select(-crashSeverity)

test_lr <- test_data
class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize severe crashes class more heavily

lr_model <- glm(crashSeverity ~ ., data = training_data, weights=class_weights, family = "binomial")

#print(lr_model)
predictions <- predict(lr_model, newdata = test_lr, type = "response")
binary_predictions <- factor(ifelse(predictions >= 0.5, 1, 0), levels = levels(test_Y))
# accuracy <- mean(binary_predictions == test_Y)
test_Y <- factor(test_Y)

confusion_matrix <- confusionMatrix(binary_predictions, test_Y)
accuracy <- accuracy <- confusion_matrix$overall["Accuracy"]
print(confusion_matrix$table)
#print(confusion_matrix)

f_score <- confusion_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)

# # Extract coefficients
# coefficients <- coef(lr_model)
# 
# # Calculate absolute values of coefficients
# abs_coefficients <- abs(coefficients)
# 
# # Create a dataframe to store coefficients and their absolute values
# coef_df <- data.frame(predictor = names(coefficients), coefficient = coefficients, abs_coefficient = abs_coefficients)
# 
# # Sort coefficients based on absolute values
# sorted_coef_df <- coef_df[order(abs_coefficients, decreasing = TRUE), ]
# 
# # Print sorted coefficients
# knitr::kable(head(sorted_coef_df,15))

importance <- varImp(lr_model, scale = FALSE)

variable_names <- rownames(importance)
#print(variable_names)

importance_df <- data.frame(importance)
#glimpse(importance_df)
importance_scores <- importance[, 1]

# Create a data frame with variable names and importance scores
importance_df <- data.frame(
  Variable = variable_names,
  Importance = importance_scores
)

# Convert Importance column to numeric
importance_df$Importance <- as.numeric(as.character(importance_df$Importance))

# Sort the data frame by Importance column in descending order
importance_df_sorted <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Print the sorted importance scores
importance_df_sorted <- as.data.frame(importance_df_sorted)
write.csv(importance_df_sorted, "importance_glm.csv", row.names = FALSE)

#knitr::kable(head(importance_df_sorted,15))

```


```{r echo=FALSE,cache=TRUE,warning=FALSE,fig.width=6, fig.height=4}
# # Fit the logistic regression model
# #lr_model <- glm(crashSeverity ~ ., data = training_data, family = "binomial")
# 
# # Get the summary of the model
# model_summary <- summary(lr_model)
# 
# # Check if the summary object is correct
# #print(model_summary)
# 
# # Extract coefficients and p-values
# coefficients <- model_summary$coefficients
# p_values <- coefficients[, "Pr(>|z|)"]
# 
# # Create a data frame with predictors and their p-values
# predictors_pvalues <- data.frame(
#   Predictor = rownames(coefficients),
#   P_Value = p_values
# )
# 
# # Sort by p-value
# sorted_predictors <- predictors_pvalues %>%
#   arrange(P_Value)
# 
# # Print sorted predictors by p-value


 #knitr::kable(head(as.data.frame(sorted_predictors),15))
# # knitr::kable(head(as.data.frame(sorted_predictors),15))

```

### 2. fit decision tree model with random forest ensemble with permutation
Permutation importance provide a more accurate estimate of variable importance, especially in situations where the relationship between predictors and the response is nonlinear or non-monotonic.


using     


class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize 'setosa' class more heavily.    


cv_rf <- ranger(crashSeverity ~ ., data = training_data, num.trees = 500,    
                   mtry = 6,
                  min.node.size = 3, 
                  case.weights = class_weights,
                  importance = "permutation",
                  sample.fraction = 0.8,
                  num.fold = 5, 
              verbose = FALSE
        ).   
        

Ouput the confusion matrix,F1 score and  accuracy of the prediction. Then write the importance list to file  importance_rf.csv.


```{r echo=FALSE,cache=TRUE,warning=FALSE}
# library(ranger)
# 
# test_Y <- test_data$crashSeverity
# 
# #test_X <- test_data %>% select(-crashSeverity)
# 
# 
# #rf_model <- ranger(crashSeverity ~ ., data = training_data,importance = "impurity")
# rf_model <- ranger(crashSeverity ~ ., data = training_data,importance = "impurity")
# 
# #summary(rf_model)
# 
# predictions <- predict(rf_model, data = test_data)
# 
# predicted_values <- predictions$predictions
# knitr::kable(table(predicted_values,test_Y))
# 
# accuracy <- mean(predicted_values == test_Y)
# 
# print("Accuracy is:")
# print(accuracy)
# 
# importance_measures <- importance(rf_model)
# 
# #importance_df <-data.frame(name = names(importance_measures),value = unlist(importance_measures))
# importance_df <-data.frame(importance_measures)
# 
# #glimpse(importance_df)
# sorted_importance_df <- importance_df[order(importance_df$importance_measures,decreasing = TRUE),,drop = FALSE]
# 
#  knitr::kable(head(sorted_importance_df,15))
#  
# rf_model2 <- ranger(crashSeverity ~ ., data = training_data,
#     num.trees = 500,
#     mtry = 6,
#     min.node.size = 3,
#     sample.fraction = 0.8,
#     seed = 230,
#   importance = "impurity"
# )
# 
# predictions2 <- predict(rf_model2, data = test_data)
# 
# predicted_values2 <- predictions2$predictions
# knitr::kable(table(predicted_values2,test_Y))
# 
# accuracy <- mean(predicted_values2 == test_Y)
# 
# print("Accuracy is:")
# print(accuracy)
# 
# importance_measures2 <- importance(rf_model2)
# 
# #importance_df <-data.frame(name = names(importance_measures),value = unlist(importance_measures))
# importance_df2 <-data.frame(importance_measures2)
# 
# #glimpse(importance_df)
# sorted_importance_df2 <- importance_df2[order(importance_df2$importance_measures,decreasing = TRUE),,drop = FALSE]
# 
#  knitr::kable(head(sorted_importance_df2,15))
```

```{r echo=FALSE,cache=TRUE,warning=FALSE}
library(ranger)
# Define class weights
class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize 'setosa' class more heavily

# Cross-validation with ranger
cv_rf <- ranger(crashSeverity ~ ., data = training_data, num.trees = 500,    
                   mtry = 6,
                  min.node.size = 3, 
                  case.weights = class_weights,
#                  importance = "impurity",
                importance = "permutation",
                 sample.fraction = 0.8,
                  num.fold = 5,  # Number of folds for cross-validation
              verbose = FALSE  # Print progress
        )

# Get cross-validation results
#cv_results <- cv_rf$prediction.error

#print(cv_results)
# Find the fold with the lowest prediction error
#best_fold <- which.min(cv_results)

# Get the corresponding model
#best_model <- cv_rf

# Print information about the best model
#print(best_model)

predictions <- predict(cv_rf, data = test_data)

predicted_values <- predictions$predictions

# accuracy <- mean(binary_predictions == test_Y)
test_Y <- factor(test_Y)

confusion_matrix <- confusionMatrix(predicted_values, test_Y)
accuracy <- confusion_matrix$overall["Accuracy"]
print(confusion_matrix$table)
#print(confusion_matrix)

f_score <- confusion_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)

importance_measures <- importance(cv_rf)

# Convert the named numeric vector to a dataframe
importance_df <- data.frame(
  Feature = names(importance_measures),
  Importance = as.numeric(importance_measures)
)


#importance_df <-data.frame(name = names(importance_measures),value = unlist(importance_measures))
#importance_df <-data.frame(importance_measures)

#glimpse(importance_df)
sorted_importance_df <- importance_df[order(importance_df$Importance,decreasing = TRUE),,drop = FALSE]

write.csv(sorted_importance_df, "importance_rf.csv", row.names = FALSE)
# knitr::kable(head(sorted_importance_df,15))
```

```{r echo=FALSE,cache=TRUE,warning=FALSE}
# Load the tuneRanger package
# library(ranger)
# library(caret)
# 
# # Define parameter grid
# # Define parameter grid
# param_grid <- expand.grid(
#   mtry = c(2, 3,4),
#   splitrule = c("gini", "extratrees"),
#   min.node.size = c(1, 5, 10)
# )
# 
# # Perform grid search with cross-validation
# ctrl <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation
# rf_grid <- caret::train(
#  crashSeverity ~ .,
#   data = training_data,
#   method = "ranger", 
#   trControl = ctrl,
#   tuneGrid = param_grid
# )
# 
# 
# # Get best parameters
# best_params <- rf_grid$bestTune
```


```{r echo=FALSE,cache=TRUE,warning=FALSE}
# class_weights <- ifelse(training_data$crashSeverity == 1, 3, 1)  # Penalize 'setosa' class more heavily
# 
# 
# # Train final model with best parameters
# final_model <- ranger(
#   crashSeverity ~ .,
#   data = training_data,
#   num.trees = 200,
#   mtry = best_params$mtry,
#   min.node.size = best_params$min.node.size,
#                     case.weights = class_weights
# 
# )
# 
# 
# 
# predictions <- predict(final_model, data = test_data)
# 
# predicted_values <- predictions$predictions
# 
# # accuracy <- mean(binary_predictions == test_Y)
# test_Y <- factor(test_Y)
# 
# confusion_matrix <- confusionMatrix(predicted_values, test_Y)
# accuracy <- confusion_matrix$overall["Accuracy"]
# print(conf_matrix$table)
# #print(confusion_matrix)
# 
# f_score <- confusion_matrix$byClass["F1"]
# 
# print(f_score)
# 
# #knitr::kable(table(binary_predictions,test_Y))
# 
# print(accuracy)
# 
# importance_measures <- importance(best_model)
# 
# #importance_df <-data.frame(name = names(importance_measures),value = unlist(importance_measures))
# importance_df <-data.frame(importance_measures)
# 
# #glimpse(importance_df)
# sorted_importance_df <- importance_df[order(importance_df$importance_measures,decreasing = TRUE),,drop = FALSE]
# 
#  knitr::kable(head(sorted_importance_df,15))
```


### 3. Using xgboost ensemble with logistic regression to predict.
Tried to use CV to find the best hyper-paramters of xgboost, but need too long time to run in my computer.So interupted and just use the parameters like the following.

class_weights <- ifelse(training_data$crashSeverity == 1,2, 1)  # Penalize 'setosa' class more heavily.   

positive_weight <- sum(class_weights[training_Y == 1]) / sum(class_weights[training_Y == 0]).   

xgb_model <- xgboost(data = as.matrix(training_X), label = training_Y,
                     max_depth = 3, eta = 0.1, nrounds = 300, 
                     scale_pos_weight = positive_weight,  # Set scale_pos_weight
                     objective = "binary:logistic",
                     verbose = FALSE).   

    
Ouput the confusion matrix,F1 score and  accuracy of the prediction. Then write the importance list to importance_xgb.csv.


```{r echo=FALSE,cache=TRUE,warning=FALSE}
library(xgboost)

library(caret)

test_Y <- test_data$crashSeverity
test_Y <-  as.numeric(test_Y)-1
test_X <- test_data %>% select(-crashSeverity)

training_X <- training_data %>% select(-crashSeverity)

training_Y <- training_data$crashSeverity

#X <- as.matrix(training_X)  # Features
training_Y <- as.numeric(training_Y)-1
#table(training_Y)

class_weights <- ifelse(training_data$crashSeverity == 1,2, 1)  # Penalize 'setosa' class more heavily
positive_weight <- sum(class_weights[training_Y == 1]) / sum(class_weights[training_Y == 0])

# Train the XGBoost model
xgb_model <- xgboost(data = as.matrix(training_X), label = training_Y,
                     max_depth = 4, eta = 0.1, nrounds = 300, 
                     scale_pos_weight = positive_weight,  # Set scale_pos_weight
                     objective = "binary:logistic",
                     verbose = FALSE)

# Predict probabilities
predictions <- predict(xgb_model, as.matrix(test_X))  # Probability of positive class

# Convert predictions to factor with levels "0" and "1"
predictions <- factor(ifelse(predictions >= 0.5, 1, 0))

# Convert test_Y to factor with levels "0" and "1"
test_Y <- factor(test_Y)

# Compute confusion matrix
conf_matrix <- confusionMatrix(predictions, test_Y)
print(conf_matrix$table)

accuracy <- conf_matrix$overall["Accuracy"]

#print(confusion_matrix)

f_score <- conf_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)
importance_scores <- xgb.importance(model = xgb_model)

importance_scores <- as.data.frame(importance_scores)
write.csv(importance_scores, "importance_xgb.csv", row.names = FALSE)

# Print the importance scores
 
#knitr::kable(head(importance_scores,15))

# Plot feature importance
```

```{r echo=FALSE,cache=TRUE,warning=FALSE}



# # Load necessary packages
# library(caret)
# library(xgboost)
# 
# 
# 
# # Define parameter grid
# param_grid <- expand.grid(
#   nrounds = c(100, 200, 300),
#   max_depth = c(3, 5, 7),
#   eta = c(0.01, 0.1, 0.2),
#   gamma = c(0, 0.1, 0.2),
#   colsample_bytree = c(0.6, 0.8, 1),
#   min_child_weight = c(1, 3, 5),
#   subsample = c(0.6, 0.8, 1)
# )
# 
# # Define control settings
# ctrl <- trainControl(
#   method = "cv",
#   number = 5,  # Number of folds for cross-validation
#   verboseIter = TRUE
# )
# 
# 
# # Tune hyperparameters
# tuned_model <- train(
#    crashSeverity ~ .,
#    data = training_data, 
#   method = "xgbTree",
#   trControl = ctrl,
#   tuneGrid = param_grid
# )
# 
# # Print the best model
# print(tuned_model)
# 
# # Access the trained XGBoost model with the best hyperparameters
# best_xgb_model <- tuned_model$finalModel
# 
# # Make predictions using the best XGBoost model
# predictions <- predict(best_xgb_model, newdata = test_data)
# 
# # Print the predictions
# # Convert predictions to factor with levels "0" and "1"
# predictions <- factor(ifelse(predictions >= 0.5, 1, 0))
# 
# # Convert test_Y to factor with levels "0" and "1"
# test_Y <- factor(test_Y)
# 
# # Compute confusion matrix
# conf_matrix <- confusionMatrix(predictions, test_Y)
# print(conf_matrix$table)
# 
# accuracy <- conf_matrix$overall["Accuracy"]
# 
# #print(confusion_matrix)
# 
# f_score <- conf_matrix$byClass["F1"]
# 
# print(f_score)
# 
# #knitr::kable(table(binary_predictions,test_Y))
# 
# print(accuracy)
# importance_scores <- xgb.importance(model = best_xgb_model)
# 
# importance_scores <- as.data.frame(importance_scores)
# write.csv(importance_scores, "importance_xgb.csv", row.names = FALSE)
# 

```




#### 4. Also tried others like neural network to predict.Required more computaional resources but the result is no better then random forest. And the importance of the factors is not convient to get during the process of modeling.So abandoned.


```{r echo=FALSE,cache=TRUE,warning=FALSE}
# library(keras3)
# library(caret)
# test_Y <- test_data$crashSeverity
# 
# test_Y <-  as.numeric(test_Y)-1
# 
# test_X <- test_data %>% select(-crashSeverity)
# 
# training_X <- training_data %>% select(-crashSeverity)
# 
# training_Y <- training_data$crashSeverity
# 
# #X <- as.matrix(training_X)  # Features
# training_Y <- as.numeric(training_Y)-1
# 
# X_train_matrix <- training_X %>% as.matrix() %>% scale() 
# X_test_matrix <- test_X %>% as.matrix() %>% scale()
# 
# class_weights <- c("0" = 1, "1" = 3)
# 
# model <- keras_model_sequential() %>%
# layer_dense(units = 64, activation = "relu", input_shape = dim(X_train_matrix)[[2]]) %>% 
# layer_dense(units = 64, activation = "elu") %>% 
# layer_dense(units = 1, activation = "sigmoid")
# weighted_ratio <- class_weights["1"] / class_weights["0"]
# 
# # Compile the model with additional metrics
# # Compile the model with precision, recall, and AUC metrics
# model %>% compile(
#   loss = 'binary_crossentropy',
#   optimizer = optimizer_adam(),
#   metrics = c('accuracy', metric_auc(), metric_precision(), metric_recall()),
#   loss_weights = class_weights
# )
# 
# # Train the model
#  model %>% fit(X_train_matrix, training_Y, epochs = 40, batch_size = 400, validation_split = 0.2)
# 
# # Summary of the model
# summary(model)
# 
# 
# 
# # Make predictions
# predictions <- model %>% predict(X_test_matrix)
# 
# # Convert probabilities to binary class labels
# predicted_classes <- ifelse(predictions > 0.5, 1, 0)
# test_Y <- factor(test_Y, levels = c(0, 1))
# 
# # Evaluate predictions
# conf_matrix <- confusionMatrix(factor(predicted_classes), test_Y)
# print(conf_matrix)

```


```{r echo=FALSE}
# library(glmnet)
# 
# 
# elastic_net_model <- cv.glmnet(as.matrix(training_X), training_Y, family = "binomial", alpha = 0,
#                                 type.measure = "class" 
#                                )
# 
#                                
# best_lambda <- elastic_net_model$lambda.min
# print(best_lambda)
# 
# # Make predictions on the training data
# predictions <- predict(elastic_net_model, newx =as.matrix( test_X), s = best_lambda, type = "class")
# 
# # Confusion Matrix
# confusion_matrix <- table(predictions, test_Y)
# print(confusion_matrix)
# 
# # Calculate accuracy
# accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# print(paste("Accuracy:", round(accuracy, 4)))
# 
# # Extract coefficients for the best lambda
# coefficients <- coef(elastic_net_model, s = best_lambda)
# 
# # Convert the sparse matrix to a data frame
# coefficients_df <- as.data.frame(as.matrix(coefficients))
# coefficients_df$feature <- rownames(coefficients_df)
# 
# # Remove the intercept
# coefficients_df <- coefficients_df[coefficients_df$feature != "(Intercept)", ]
# 
# # Sort by the absolute value of the coefficient
# coefficients_df <- coefficients_df[order(abs(coefficients_df[, 1]), decreasing = TRUE), ]
# 
# # Rename the columns for clarity
# names(coefficients_df) <- c("coefficient", "feature")
# 
# # Display the sorted coefficients
# print(coefficients_df)


```

## Result.   


  compared with three models, the random forest got the F1 score 0.6804665 accuracy 0.726. While logistic regression got F1 score 0.6739194 Accuracy 0.719,and Xgboost got F1 0.6777761 and 0.726875. They got different importance lists. While random forest and xgboost is better but similar performance. I'd like to merge the importance list gotten from these two model to get a merged importance list. If a feature is deemed important by both models, it’s likely that the feature is truly important. This can make your interpretation more robust. The code will be in ***Appendix 3***. 
  
```{r echo=FALSE,cache=TRUE,warning=FALSE}
 
# Load the data
importance_rf <- read.csv("importance_rf.csv")
importance_xgb <- read.csv("importance_xgb.csv")

# Normalize the importance
importance_rf$Importance <- importance_rf$Importance / sum(importance_rf$Importance)
importance_xgb$Gain <- importance_xgb$Gain / sum(importance_xgb$Gain)

# Combine the importance from both models
combined_importance <- merge(importance_rf, importance_xgb, by = "Feature")
combined_importance$Combined <- combined_importance$Importance + combined_importance$Gain

# Sort the features based on this combined importance
combined_importance <- combined_importance[order(-combined_importance$Combined), ]

# Write the dataframe to a CSV file
write.csv(combined_importance, file = "merged_importance.csv", row.names = FALSE)

```

### The top 15 key factors will be:    

***carStationWagon***.       ***speedLimit***.       ***motorcycle***.       ***pedestrian***.       ***bicycle***.    ***region***.       ***roadLane***.       ***crashFinancialYear***.       ***streetLight***.       ***tree***.    ***vanOrUtility***.       ***fence***.       ***weather***.       ***NumberOfLanes***.       ***postOrPole***.   

Btw, the full sorted list of every model will be in the ***appendix 1***. 

## Discussion.   


1. ***There*** are 767290 regular crashes and  54454 severe crashes.  Our target is to find the cause of severe crashes,but the severe observations' size is really small compares to the regular crashes. I decisied to equally sample from severe crashes and regular crashes,that is ***20000*** observations from each type of crashes. To find the key factors of severe crashes, I put 2 times heaver penality for misclassfying the server crashes.    


2. The Data provided is not only imbalance in  the target class,but also very unbalance in predictors(as you can see in the plot of bicycles).     


3. To analysis the sampled data,as we can see the with the simlilar attribute value, having different target lable. It makes the inference be different, and indicates low accuracy of prediction.    



4. I fitted the model with different method( logistic, random forest and xgboost). It turned out that the random forest and xgboost had similar performance, the xgboost given higher TPR.    


5. Permutation importance provide a more accurate estimate of variable importance, especially in situations where the relationship between predictors and the response is nonlinear or non-monotonic.So that is used to build Random Forest here.    



6. Tried to find best hyper parameters for random forest and xgboost, but very time consuming, and the final outcomes no outstanding improvement compared to the current variable. Some related code still keep in the source code,but ommited.    


7. Tried more heavier penality, but the accuracy( (TP+TN)/TOTAL) lower than 70%, using 2 times heavier finnally.    


8.  Random forest and xgboost have similar performance,but got different importance lists. Combine two importance after normalizing provide more robust interpration for the feature is deemed important by both models(Appendix 3).

9. All the code(used finnally ) related to the three models are in the Appendix 2.

\newpage


## Appendix.   

### Appendix 1.  The importance lists from all the models and combined:     




```{r echo=FALSE,warning=FALSE}
# Install the knitr package if not already installed
# install.packages("knitr")

# Load the knitr package
library(knitr)
library(tidyverse)

# Load the data
data <- read_csv("./importance_compare.csv")

# Create a table using kable
kable(data)

```
### Appendix 2.  Codes of models:    



***1. For logistic regression:***
```r
library(caret)


test_Y <- test_data$crashSeverity

#test_X <- test_data %>% select(-crashSeverity)

test_lr <- test_data
class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize severe crashes class more heavily

lr_model <- glm(crashSeverity ~ ., data = training_data, weights=class_weights, family = "binomial")

#print(lr_model)
predictions <- predict(lr_model, newdata = test_lr, type = "response")
binary_predictions <- factor(ifelse(predictions >= 0.5, 1, 0), levels = levels(test_Y))
# accuracy <- mean(binary_predictions == test_Y)
test_Y <- factor(test_Y)

confusion_matrix <- confusionMatrix(binary_predictions, test_Y)
accuracy <- accuracy <- confusion_matrix$overall["Accuracy"]
print(confusion_matrix$table)
#print(confusion_matrix)

f_score <- confusion_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)

# # Extract coefficients
# coefficients <- coef(lr_model)
# 
# # Calculate absolute values of coefficients
# abs_coefficients <- abs(coefficients)
# 
# # Create a dataframe to store coefficients and their absolute values
# coef_df <- data.frame(predictor = names(coefficients), coefficient = coefficients, abs_coefficient = abs_coefficients)
# 
# # Sort coefficients based on absolute values
# sorted_coef_df <- coef_df[order(abs_coefficients, decreasing = TRUE), ]
# 
# # Print sorted coefficients
# knitr::kable(head(sorted_coef_df,15))

importance <- varImp(lr_model, scale = FALSE)

variable_names <- rownames(importance)
#print(variable_names)

importance_df <- data.frame(importance)
#glimpse(importance_df)
importance_scores <- importance[, 1]

# Create a data frame with variable names and importance scores
importance_df <- data.frame(
  Variable = variable_names,
  Importance = importance_scores
)

# Convert Importance column to numeric
importance_df$Importance <- as.numeric(as.character(importance_df$Importance))

# Sort the data frame by Importance column in descending order
importance_df_sorted <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

# Print the sorted importance scores
importance_df_sorted <- as.data.frame(importance_df_sorted)
write.csv(importance_df_sorted, "importance_glm.csv", row.names = FALSE)

#knitr::kable(head(importance_df_sorted,15))

```

***2. For Random Forest:***

```r
library(ranger)
# Define class weights
class_weights <- ifelse(training_data$crashSeverity == 1, 2, 1)  # Penalize 'setosa' class more heavily

# Cross-validation with ranger
cv_rf <- ranger(crashSeverity ~ ., data = training_data, num.trees = 500,    
                   mtry = 6,
                  min.node.size = 3, 
                  case.weights = class_weights,
#                  importance = "impurity",
                importance = "permutation",
                 sample.fraction = 0.8,
                  num.fold = 5,  # Number of folds for cross-validation
              verbose = FALSE  # Print progress
        )

# Get cross-validation results
#cv_results <- cv_rf$prediction.error

#print(cv_results)
# Find the fold with the lowest prediction error
#best_fold <- which.min(cv_results)

# Get the corresponding model
#best_model <- cv_rf

# Print information about the best model
#print(best_model)

predictions <- predict(cv_rf, data = test_data)

predicted_values <- predictions$predictions

# accuracy <- mean(binary_predictions == test_Y)
test_Y <- factor(test_Y)

confusion_matrix <- confusionMatrix(predicted_values, test_Y)
accuracy <- confusion_matrix$overall["Accuracy"]
print(confusion_matrix$table)
#print(confusion_matrix)

f_score <- confusion_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)

importance_measures <- importance(cv_rf)

# Convert the named numeric vector to a dataframe
importance_df <- data.frame(
  Feature = names(importance_measures),
  Importance = as.numeric(importance_measures)
)


#importance_df <-data.frame(name = names(importance_measures),value = unlist(importance_measures))
#importance_df <-data.frame(importance_measures)

#glimpse(importance_df)
sorted_importance_df <- importance_df[order(importance_df$Importance,decreasing = TRUE),,drop = FALSE]

write.csv(sorted_importance_df, "importance_rf.csv", row.names = FALSE)
# knitr::kable(head(sorted_importance_df,15))
```

***3. For Xgboost:    

```r
library(xgboost)

library(caret)

test_Y <- test_data$crashSeverity
test_Y <-  as.numeric(test_Y)-1
test_X <- test_data %>% select(-crashSeverity)

training_X <- training_data %>% select(-crashSeverity)

training_Y <- training_data$crashSeverity

#X <- as.matrix(training_X)  # Features
training_Y <- as.numeric(training_Y)-1
#table(training_Y)

class_weights <- ifelse(training_data$crashSeverity == 1,2, 1)  # Penalize 'setosa' class more heavily
positive_weight <- sum(class_weights[training_Y == 1]) / sum(class_weights[training_Y == 0])

# Train the XGBoost model
xgb_model <- xgboost(data = as.matrix(training_X), label = training_Y,
                     max_depth = 4, eta = 0.1, nrounds = 300, 
                     scale_pos_weight = positive_weight,  # Set scale_pos_weight
                     objective = "binary:logistic",
                     verbose = FALSE)

# Predict probabilities
predictions <- predict(xgb_model, as.matrix(test_X))  # Probability of positive class

# Convert predictions to factor with levels "0" and "1"
predictions <- factor(ifelse(predictions >= 0.5, 1, 0))

# Convert test_Y to factor with levels "0" and "1"
test_Y <- factor(test_Y)

# Compute confusion matrix
conf_matrix <- confusionMatrix(predictions, test_Y)
print(conf_matrix$table)

accuracy <- conf_matrix$overall["Accuracy"]

#print(confusion_matrix)

f_score <- conf_matrix$byClass["F1"]

print(f_score)

#knitr::kable(table(binary_predictions,test_Y))

print(accuracy)
importance_scores <- xgb.importance(model = xgb_model)

importance_scores <- as.data.frame(importance_scores)
write.csv(importance_scores, "importance_xgb.csv", row.names = FALSE)

# Print the importance scores
 
#knitr::kable(head(importance_scores,15))

# Plot feature importance
```

### Appendix 3.  Code to Combine the importance list:     


```r
 
# Load the data
importance_rf <- read.csv("importance_rf.csv")
importance_xgb <- read.csv("importance_xgb.csv")

# Normalize the importance
importance_rf$Importance <- importance_rf$Importance / sum(importance_rf$Importance)
importance_xgb$Gain <- importance_xgb$Gain / sum(importance_xgb$Gain)

# Combine the importance from both models
combined_importance <- merge(importance_rf, importance_xgb, by = "Feature")
combined_importance$Combined <- combined_importance$Importance + combined_importance$Gain

# Sort the features based on this combined importance
combined_importance <- combined_importance[order(-combined_importance$Combined), ]

# Write the dataframe to a CSV file
write.csv(combined_importance, file = "merged_importance.csv", row.names = FALSE)

```


### Appendix 4.  All The Attributes description:    

```{r echo=FALSE,warning=FALSE}
# Install the knitr package if not already installed
# install.packages("knitr")

# Load the knitr package
library(knitr)
library(readxl)

# Load the data
data <- read_excel("./Attributes_description.xlsx")

# Create a table using kable
kable(data)

```
